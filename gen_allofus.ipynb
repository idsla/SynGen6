{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# presteps: install packages and restart the kernel\n",
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import os \n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy, scipy\n",
    "import matplotlib, matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from datetime import date\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import laplace\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "from copy import deepcopy\n",
    "\n",
    "bucket = os.getenv(\"WORKSPACE_BUCKET\") # my work dir\n",
    "dataset = os.getenv(\"WORKSPACE_CDR\")\n",
    "genomic_location = os.getenv(\"CDR_STORAGE_PATH\")\n",
    "\n",
    "print(bucket)\n",
    "print(dataset)\n",
    "print(genomic_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLINK Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy files from bucket to local\n",
    "!gsutil -u $GOOGLE_PROJECT cp gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/ancestry/ancestry_preds.tsv .\n",
    "!gsutil -u $GOOGLE_PROJECT cp gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/aux/relatedness/relatedness_flagged_samples.tsv .\n",
    "!gsutil -u ${GOOGLE_PROJECT} ls gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel\n",
    "    \n",
    "!mkdir -p plink2\n",
    "!gsutil -u $GOOGLE_PROJECT -m cp gs://fc-aou-datasets-controlled/v7/wgs/short_read/snpindel/clinvar_v7.1/plink_bed/* plink2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for chr in {13,14,20,21,4,8}; do \n",
    "    plink --bfile plink2/clinvar.chr${chr} \\\n",
    "          --maf 0.01 \\\n",
    "          --geno 0.05 \\\n",
    "          --mind 0.05 \\\n",
    "          --hwe 1e-6 \\\n",
    "          --recode AD \\\n",
    "          --out genotype_data_qc${chr}\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load genotype data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrs = [13,14,20,21,4,8]\n",
    "dfs = []\n",
    "dfs_filter = []\n",
    "relatedness = pd.read_csv(\"relatedness_flagged_samples.tsv\", delimiter=\"\\t\")\n",
    "print(relatedness.shape)\n",
    "relatedness.columns = ['IID']\n",
    "for chr_ in chrs:\n",
    "    print(f\"Chr Data reading {chr_}...\")\n",
    "    df = pd.read_csv(f'./genotype_data_qc{chr_}.raw', delimiter=' ')\n",
    "    df_filtered = df[~df['IID'].isin(relatedness['IID'].values.tolist())]\n",
    "    print(df.shape)\n",
    "    print(df_filtered.shape)\n",
    "    dfs.append(df)\n",
    "    dfs_filter.append(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge processed data for all chromosomes\n",
    "\n",
    "common_iids = set(dfs_filter[0]['IID'])\n",
    "for df in dfs_filter[1:]:\n",
    "    common_iids.intersection_update(df['IID'])\n",
    "    \n",
    "common_iids = list(common_iids)\n",
    "print(len(common_iids))\n",
    "\n",
    "filtered_dfs = [df[df['IID'].isin(common_iids)].reset_index(drop = True) for df in dfs_filter]\n",
    "\n",
    "# snp data\n",
    "snp_data = pd.concat([item.iloc[:, 6:] for item in filtered_dfs], axis = 1)\n",
    "print(snp_data.shape)\n",
    "\n",
    "# meta data\n",
    "meta_data = filtered_dfs[0].loc[:, ['IID', 'SEX', 'FID']]\n",
    "print(snp_data.shape, meta_data.shape)\n",
    "\n",
    "# final data\n",
    "final_data = pd.concat([meta_data, snp_data], axis = 1)\n",
    "print(final_data.shape)\n",
    "\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ancestry data\n",
    "\n",
    "start = time.time()\n",
    "ancestry_pred = pd.read_csv(\"ancestry_preds.tsv\", delimiter=\"\\t\")\n",
    "final_data = pd.merge(\n",
    "    final_data.set_index('IID'), ancestry_pred.set_index('research_id')[['ancestry_pred']], \n",
    "    how = 'inner', left_index = True, right_index = True\n",
    ")\n",
    "final_data = final_data.drop(['SEX', 'FID'], axis = 1)\n",
    "columns = final_data.columns[-1:].tolist() + final_data.columns[:-1].tolist()\n",
    "final_data = final_data[columns]\n",
    "\n",
    "final_data.rename(columns={'ancestry_pred': 'population'},inplace = True)\n",
    "end = time.time()\n",
    "print(f'{end - start}s')\n",
    "final_data.to_csv('final_data_filtered.csv', index = False)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic SNPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X:np.array, k = 6):\n",
    "    '''\n",
    "    Preprocessing SNPs data using standard scaler and PCA and get Kmeans model\n",
    "    \n",
    "    X: (n_users, n_snps) - numpy array\n",
    "    k: number of clusters\n",
    "    '''\n",
    "    # fill missing data with 0\n",
    "    X[np.isnan(X)] = 0\n",
    "    print(X.shape)\n",
    "\n",
    "    # standard scaler\n",
    "    start = time.time()\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    end = time.time()\n",
    "    print(f'{end - start}s')\n",
    "\n",
    "    # pca on whole data\n",
    "    start = time.time()\n",
    "    pca = PCA(n_components=d, random_state=seed)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    print(X_pca.shape)\n",
    "    end = time.time()\n",
    "    print(f'{end - start}s')\n",
    "    joblib.dump(pca, 'pca_model.pkl')\n",
    "\n",
    "    # kmeans \n",
    "    start = time.time()\n",
    "    kmeans = KMeans(n_clusters=k, random_state=seed, n_init = 'auto')\n",
    "    kmeans.fit(X_pca)\n",
    "    end = time.time()\n",
    "    print(f'{end - start}s')\n",
    "    joblib.dump(kmeans, 'kmeans_model.pkl')\n",
    "    \n",
    "    return scaler, pca, kmeans, X_pca\n",
    "\n",
    "def randomized_response(val, p, q):\n",
    "    '''\n",
    "    Add randomized response to the SNPs data\n",
    "    \n",
    "    val: int\n",
    "    p: float\n",
    "    q: float\n",
    "    '''\n",
    "    rand_val = np.random.uniform(0, 1)\n",
    "    new_val = val\n",
    "    if rand_val > p:\n",
    "        if val == 0:\n",
    "            new_val = 1\n",
    "        elif val == 2:\n",
    "            new_val = 1\n",
    "        elif val == 1:\n",
    "            if rand_val > p+q:\n",
    "                new_val = 0\n",
    "            else:\n",
    "                new_val = 2\n",
    "    return new_val\n",
    "\n",
    "\n",
    "def add_noise_shuffle(X_sample: np.array, p, q, seed = 21):\n",
    "    '''\n",
    "    Add randomized response to the SNPs data by the following steps:\n",
    "    1. Generate a random uniform random array\n",
    "    2. Apply randomized response to the SNPs data\n",
    "    3. Handle 0 and 2 with probability p to be 1\n",
    "    4. Handle 1 with probability p+q to be 0 and 1 - p - q to be 2\n",
    "    \n",
    "    X_sample: (n_users, n_snps) - numpy array\n",
    "    p: float\n",
    "    q: float\n",
    "    seed: int\n",
    "    '''\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # random uniform random array\n",
    "    rand_matrix = np.random.uniform(0, 1, size=X_sample.shape)\n",
    "    \n",
    "    # generated matrix\n",
    "    X_sample_noise = X_sample.copy()\n",
    "    \n",
    "    # apply randomized response\n",
    "    \n",
    "    # handle 0 and 2\n",
    "    mask_0_2 = (X_sample == 0) | (X_sample == 2)\n",
    "    X_sample_noise[mask_0_2 & (rand_matrix > p)] = 1\n",
    "    \n",
    "    # handle 1\n",
    "    mask_1 = X_sample == 1\n",
    "    X_sample_noise[mask_1 & (rand_matrix > p) & (rand_matrix > p+q)] = 0\n",
    "    X_sample_noise[mask_1 & (rand_matrix > p) & (rand_matrix <= p+q)] = 2\n",
    "    \n",
    "    return X_sample_noise\n",
    "\n",
    "def validation(X_sample_real, X_sample_gen, scaler, pca, kmeans):\n",
    "    '''\n",
    "    Validation of the generated SNPs data\n",
    "    1. Equality check - it should not be the same as the real data\n",
    "    2. Clustering check - it should fall into the same cluster as the real data\n",
    "    \n",
    "    X_sample_real: (n_users, n_snps) - numpy array\n",
    "    X_sample_gen: (n_users, n_snps) - numpy array\n",
    "    scaler: sklearn.preprocessing.StandardScaler\n",
    "    pca: sklearn.decomposition.PCA\n",
    "    kmeans: sklearn.cluster.KMeans\n",
    "    '''\n",
    "    \n",
    "    # equality check\n",
    "    equal_compare = np.all(X_sample_real == X_sample_gen, axis = 1)    \n",
    "    \n",
    "    # clustering check\n",
    "    #     X_sample_gen_pca = pca.transform(scaler.transform(X_sample_gen))\n",
    "    #     X_sample_real_pca = pca.transform(scaler.transform(X_sample_real))\n",
    "    X_sample_gen_pca = pca.transform(X_sample_gen)\n",
    "    X_sample_real_pca = pca.transform(X_sample_real)\n",
    "    cluster_compare = (kmeans.predict(X_sample_gen_pca) != kmeans.predict(X_sample_real_pca))\n",
    "    \n",
    "    compare = np.array([equal_compare, cluster_compare])\n",
    "    low_quality_ids = np.where(compare.transpose().any(axis=1))[0]\n",
    "    \n",
    "    return low_quality_ids\n",
    "\n",
    "def process_generated_data(gen_results):\n",
    "    '''\n",
    "    Process the generated SNPs data - merge together generated snps and sampling\n",
    "    \n",
    "    gen_results: dict   \n",
    "    '''\n",
    "    threshold = 5000\n",
    "\n",
    "    gen_results_old = deepcopy(gen_results)\n",
    "    final_gen_datas = []\n",
    "\n",
    "    for population_name, ret in gen_results_old.items():\n",
    "\n",
    "        gen_datas = []\n",
    "        for item in ret:\n",
    "            gen_datas.append(item['X_gen_final'])\n",
    "\n",
    "        gen_data = np.concatenate(gen_datas, axis = 0)\n",
    "        gen_data = pd.DataFrame(gen_data)\n",
    "        print(population_name, gen_data.shape)\n",
    "        # sampling\n",
    "        gen_data = gen_data.sample(n = threshold, random_state = 32).reset_index(drop = True)\n",
    "        gen_data['population'] = population_name\n",
    "        print(gen_data.shape)\n",
    "\n",
    "        final_gen_datas.append(gen_data)\n",
    "\n",
    "    final_gen_df = pd.concat(final_gen_datas, axis = 0)\n",
    "    \n",
    "    final_gen_df['sample_id'] = np.arange(1, final_gen_df.shape[0]+1)\n",
    "    columns = ['sample_id', 'population'] + final_gen_df.columns.tolist()[:-2]\n",
    "    final_gen_df = final_gen_df[columns]\n",
    "    print(final_gen_df.shape)\n",
    "    \n",
    "    return final_gen_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main script for generating synthetic SNPs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "d = 200\n",
    "num_sample_threshold = 5000\n",
    "seed = 21\n",
    "population_name = 'eur'\n",
    "epsilon = 60\n",
    "eps = epsilon / 10.0\n",
    "p = np.exp(eps)/(np.exp(eps)+2)\n",
    "q = 1/(np.exp(eps)+2)\n",
    "print(p, q)\n",
    "\n",
    "# Preprocessing\n",
    "seed = 21\n",
    "X = final_data.iloc[:, 1:].values.copy()\n",
    "k = final_data['population'].nunique()\n",
    "scaler, pca, kmeans, X_pca = preprocessing(X)\n",
    "\n",
    "# Generation - for each population, repeat generation multiple times until number of valid samples reach the sample threshold\n",
    "gen_results = {}\n",
    "population_group = final_data['population'].value_counts().to_dict()\n",
    "for population_name, population_count in population_group.items():\n",
    "    \n",
    "    ##########################################################################################################\n",
    "    # Generation for each population\n",
    "    ##########################################################################################################\n",
    "\n",
    "    sample_ids = final_data[final_data['population'] == population_name].reset_index(drop=True).index.tolist()\n",
    "    sample_ids_copy = sample_ids.copy()\n",
    "    print(population_name, population_count, len(sample_ids))\n",
    "    \n",
    "    total_num_samples = 0\n",
    "    run_times = 5\n",
    "    repetition_iter = 1\n",
    "    total_rets = []\n",
    "    while total_num_samples <= num_sample_threshold:\n",
    "\n",
    "        print('='*50)\n",
    "        print(f'Repetition {repetition_iter} ...')\n",
    "        repetition_iter += 1\n",
    "        seed = (seed + 10986)%((2^31)-1)\n",
    "        sample_ids = sample_ids_copy\n",
    "\n",
    "        ################################################################################\n",
    "        # Generation for one repetition\n",
    "        X_gens = []\n",
    "        sample_ids_list = []\n",
    "        low_quality_ids_list = []\n",
    "        num_valid_samples = 0\n",
    "\n",
    "        for iteration in range(run_times):\n",
    "\n",
    "            print(f'Times: {iteration}')\n",
    "\n",
    "            # generation\n",
    "            X_real_sample = final_data.iloc[:, 1:].values[sample_ids, :]\n",
    "            X_gen_sample = add_noise_shuffle(X_real_sample, p, q, seed = seed)\n",
    "\n",
    "            print(X_gen_sample.shape, X_real_sample.shape)\n",
    "\n",
    "            # validation\n",
    "            low_quality_id_indices = validation(X_gen_sample, X_real_sample, scaler, pca, kmeans)\n",
    "            sample_ids_array = np.array(sample_ids)\n",
    "            low_quality_ids = sample_ids_array[low_quality_id_indices].tolist()\n",
    "            print(len(low_quality_ids))\n",
    "\n",
    "            X_gens.append(X_gen_sample)\n",
    "            sample_ids_list.append(sample_ids)\n",
    "            low_quality_ids_list.append(low_quality_ids)\n",
    "\n",
    "            num_valid_samples += len(sample_ids) - len(low_quality_id_indices)\n",
    "            \n",
    "            if num_valid_samples > 5000 or len(low_quality_ids) == 0:\n",
    "                break\n",
    "\n",
    "            sample_ids = low_quality_ids # regenrate snps for low quality ids\n",
    "            seed = (seed + 10986)%((2^31)-1)\n",
    "\n",
    "        #############################################################################\n",
    "        # craft generated samples\n",
    "        assert len(X_gens[0]) == len(sample_ids_list[0])\n",
    "        X_gen_sample_final = X_gens[0]\n",
    "        for sample_ids_item, X_gen_item in zip(sample_ids_list[1:], X_gens[1:]):\n",
    "            X_gen_sample_final[sample_ids_item, :] = X_gen_item\n",
    "\n",
    "        X_gen_sample_final = np.delete(X_gen_sample_final, low_quality_ids_list[-1], axis = 0)\n",
    "        sample_ids_final = list(set(sample_ids_list[0]).difference(set(low_quality_ids_list[-1])))\n",
    "\n",
    "        total_num_samples += num_valid_samples\n",
    "        print(f'Current total sample: {total_num_samples}, current round gen data shape {X_gen_sample_final.shape} ({len(sample_ids_final)})')\n",
    "\n",
    "        total_rets.append({\n",
    "            'X_gens': X_gens,\n",
    "            'sample_ids_list': sample_ids_list,\n",
    "            'low_quality_ids_list': low_quality_ids_list,\n",
    "            'X_gen_final': X_gen_sample_final.copy()\n",
    "        })\n",
    "\n",
    "    print(total_num_samples)\n",
    "    gen_results[population_name] = total_rets\n",
    "    \n",
    "final_gen_df = process_generated_data(gen_results)\n",
    "final_gen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gen_df.to_csv('./allofus_snps.csv', index = False)\n",
    "final_data.reset_index(drop = True).to_csv('./allofus_snps_real.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenotype Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following script need to be run in R envronment, First change the kernel to R and then run the following script\n",
    "'''\n",
    "\n",
    "if (!require(\"BiocManager\", quietly = TRUE))\n",
    " install.packages(\"BiocManager\")\n",
    "\n",
    "BiocManager::install(\"snpStats\")\n",
    "\n",
    "install.packages('PhenotypeSimulator')\n",
    "install.packages('tidyverse')\n",
    "install.packages('data.table')\n",
    "\n",
    "library(tidyverse)\n",
    "library(PhenotypeSimulator)\n",
    "file_path <- \"./allofus_snps.csv\"\n",
    "snp_data <- read_csv(file_path)\n",
    "genotypes = snp_data %>% select(3:ncol(.)) %>% as.matrix()\n",
    "\n",
    "noiseBg <- noiseBgEffects(N = nrow(genotypes), P = 1)\n",
    "causalSNPs <- getCausalSNPs(N=nrow(genotypes), NrCausalSNPs = round(0.1*ncol(genotypes)),genotypes=genotypes)\n",
    "causalSNPs <- standardiseGenotypes(genotypes)\n",
    "geneticFixed <- geneticFixedEffects(N=nrow(genotypes), X_causal=causalSNPs, P=1, id_samples = 1:nrow(genotypes))\n",
    "\n",
    "genVar <- 0.9\n",
    "noiseVar <- 1- genVar\n",
    "# rescale phenotype components\n",
    "genBg_ind_scaled <- rescaleVariance(geneticFixed$independent, genVar)\n",
    "noiseBg_ind_scaled <- rescaleVariance(noiseBg$independent, noiseVar)\n",
    "\n",
    "# combine components into final phenotype\n",
    "Y <- scale(genBg_ind_scaled$component + noiseBg_ind_scaled$component)\n",
    "\n",
    "# transform to binary\n",
    "sigmoid <- function(x) {\n",
    "  1 / (1 + exp(-x))\n",
    "}\n",
    "\n",
    "Y_sigmoid <- sigmoid(Y)\n",
    "threshold <- median(Y_sigmoid)\n",
    "Y_binary <- as.integer(Y_sigmoid >= threshold)\n",
    "\n",
    "pheno_data <- snp_data %>%\n",
    "    select(SampleID) %>%\n",
    "    mutate(PhenotypeCondition = Y_binary)\n",
    "\n",
    "write_csv(pheno_data, \"./allofus_phenotype.csv\", col_names = TRUE)\n",
    "\n",
    "hist(Y_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Watermark SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_snp(n, phenotype_vector, eps, z0):\n",
    "    \"\"\"\n",
    "    Generate a synthetic SNP vector based on the provided phenotype vector.\n",
    "    \n",
    "    Parameters:\n",
    "    - n: The number of subjects (size of the phenotype vector)\n",
    "    - phenotype_vector: A numpy array of size n containing the phenotype values (0s and 1s)\n",
    "    - eps: Epsilon parameter for controlling the noise\n",
    "    - z0: Proportion factor for 0s and 1s in the phenotype vector\n",
    "    \n",
    "    Returns:\n",
    "    - synthetic_snp: A synthetic SNP vector with values in {0, 1, 2}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Initialize the synthetic SNP vector\n",
    "    synthetic_snp = phenotype_vector.copy()\n",
    "    \n",
    "    # Step 2: Find indices where phenotype is 0 (i0) and 1 (i1)\n",
    "    i0 = np.where(phenotype_vector == 0)[0]\n",
    "    i1 = np.where(phenotype_vector == 1)[0]\n",
    "    \n",
    "    # Step 3: Compute the number of 1's to be removed (or added if negative)\n",
    "    nremove1 = len(i1) - int(n * z0 * (1 - eps))\n",
    "    \n",
    "    # Step 4: Compute the number of 0's to be removed (or added if negative)\n",
    "    nremove0 = int(n * eps) - nremove1\n",
    "    \n",
    "    # Step 7: Randomly assign selected entries of s* to 2\n",
    "    # Handle cases based on nremove1 and nremove0\n",
    "    if nremove1 > 0 and nremove0 > 0:  # Case a\n",
    "        # Randomly pick nremove1 indices from i1, and nremove0 from i0, set corresponding values to 2\n",
    "        selected_i1 = np.random.choice(i1, size=nremove1, replace=False)\n",
    "        selected_i0 = np.random.choice(i0, size=nremove0, replace=False)\n",
    "        synthetic_snp[selected_i1] = 2\n",
    "        synthetic_snp[selected_i0] = 2\n",
    "    \n",
    "    elif nremove1 <= 0 and nremove0 > 0:  # Case b\n",
    "        # Randomly pick nremove0 indices from i0, and set values to 2\n",
    "        selected_i0 = np.random.choice(i0, size=nremove0, replace=False)\n",
    "        # Randomly pick |nremove1| indices from i1, set them to 1, then set others to 2\n",
    "        selected_i1 = np.random.choice(i1, size=abs(nremove1), replace=False)\n",
    "        synthetic_snp[selected_i1] = 1\n",
    "        synthetic_snp[selected_i0] = 2\n",
    "    \n",
    "    elif nremove1 > 0 and nremove0 <= 0:  # Case c\n",
    "        # Randomly pick nremove1 indices from i1, and set values to 2\n",
    "        selected_i1 = np.random.choice(i1, size=nremove1, replace=False)\n",
    "        # Randomly pick |nremove0| indices from i0, set them to 0, then set others to 2\n",
    "        selected_i0 = np.random.choice(i0, size=abs(nremove0), replace=False)\n",
    "        synthetic_snp[selected_i1] = 2\n",
    "        synthetic_snp[selected_i0] = 0\n",
    "    \n",
    "    return synthetic_snp\n",
    "\n",
    "def calculate_pvalues_logit(X, y):\n",
    "    '''\n",
    "    Calculate p-values for the logistic regression model\n",
    "    \n",
    "    X: (n_users, n_snps) - numpy array\n",
    "    y: (n_users) - numpy array\n",
    "    '''\n",
    "    # Add a constant term to the features matrix\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    # Fit the logistic regression model\n",
    "    model = sm.Logit(y, X).fit()\n",
    "    \n",
    "    # Extract p-values\n",
    "    return model.pvalues[1:]  # Exclude the constant term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "gen_data = pd.read_csv(\"./allofus_phenotype.csv\")\n",
    "gen_data.head()\n",
    "\n",
    "# Generate Watermark SNPs\n",
    "phenotype = gen_data['PhenotypeCondition']\n",
    "n_watermark = 20\n",
    "n_samples = phenotype.shape[0]\n",
    "\n",
    "watermark = np.zeros((n_samples, n_watermark))\n",
    "for i in range(n_watermark):\n",
    "    watermark[:, i] = generate_synthetic_snp(n = phenotype.shape[0], phenotype_vector=phenotype, eps = 0.3, z0 = 0.49)\n",
    "\n",
    "# logistic regression - p-value - significant\n",
    "# Calculate p-values\n",
    "p_values = calculate_pvalues_logit(watermark, phenotype)\n",
    "\n",
    "# Print raw p-values\n",
    "print(\"Raw p-values:\")\n",
    "print(p_values)\n",
    "print(p_values.mean(), p_values.std())\n",
    "\n",
    "watermark_data = np.concatenate([watermark, p_values.to_numpy().reshape(1,-1)], axis = 0)\n",
    "print(watermark_data.shape)\n",
    "watermark_df = pd.DataFrame(watermark_data, columns = [f'Watermark_SNP_{i}' for i in range(n_watermark)])\n",
    "index = phenotype_data['SampleID'].astype(str).values.tolist() + ['p-value']\n",
    "watermark_df.index = index\n",
    "\n",
    "watermark_df.to_csv('./allofus_watermarksnp.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Relatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def add_child(df, childID, fatherID, motherID):\n",
    "    '''\n",
    "    Add child SNPs data to the dataframe\n",
    "    \n",
    "    df: (n_users, n_snps) - pandas DataFrame\n",
    "    childID: str - child ID\n",
    "    fatherID: str - father ID\n",
    "    motherID: str - mother ID\n",
    "    '''\n",
    "    fatherCol = df.loc[:, fatherID].to_numpy()\n",
    "    motherCol = df.loc[:, motherID].to_numpy()\n",
    "    \n",
    "    # Initialize the child's SNP column with zeros\n",
    "    childCol = np.zeros_like(fatherCol)\n",
    "    \n",
    "    # Logic for SNP inheritance based on father and mother SNP values\n",
    "    for i in range(len(fatherCol)):\n",
    "        p = random.uniform(0, 1)\n",
    "        if fatherCol[i] == 0 and motherCol[i] == 0:\n",
    "            childCol[i] = 0\n",
    "        elif (fatherCol[i] == 0 and motherCol[i] == 1) or (fatherCol[i] == 1 and motherCol[i] == 0):\n",
    "            childCol[i] = 0 if p > 0.5 else 1\n",
    "        elif (fatherCol[i] == 0 and motherCol[i] == 2) or (fatherCol[i] == 2 and motherCol[i] == 0):\n",
    "            childCol[i] = 1\n",
    "        elif fatherCol[i] == 1 and motherCol[i] == 1:\n",
    "            if p < 0.333:\n",
    "                childCol[i] = 0\n",
    "            elif p < 0.666:\n",
    "                childCol[i] = 1\n",
    "            else:\n",
    "                childCol[i] = 2\n",
    "        elif (fatherCol[i] == 1 and motherCol[i] == 2) or (fatherCol[i] == 2 and motherCol[i] == 1):\n",
    "            childCol[i] = 1 if p > 0.5 else 2\n",
    "        elif fatherCol[i] == 2 and motherCol[i] == 2:\n",
    "            childCol[i] = 2\n",
    "    \n",
    "    # Add the child's SNP data to the DataFrame\n",
    "    df[childID] = childCol\n",
    "    return df\n",
    "\n",
    "def create_relatives(Gdata, n = 400):\n",
    "    '''\n",
    "    Create synthetic relatives\n",
    "    \n",
    "    Gdata: (n_snps, n_users) - numpy array\n",
    "    n: int - number of relatives\n",
    "    '''\n",
    "    rel_ids = []\n",
    "    \n",
    "    Gdata_n = Gdata.T.copy()\n",
    "    parent_ids = np.random.choice(Gdata_n.columns, size=n, replace=False)\n",
    "    relative_mapping = {}\n",
    "    # Step 1: 1st generation\n",
    "    F_id = parent_ids[:n//2]  # Father IDs\n",
    "    M_id = parent_ids[n//2:]  # Mother IDs\n",
    "    child_ids_1C = ['1C_' + str(i) for i in range(n//2)]  # Generate 100 child IDs\n",
    "    for c_id, f_id, m_id in zip(child_ids_1C, F_id, M_id):\n",
    "        Gdata_n = add_child(Gdata_n, c_id, f_id, m_id)\n",
    "        relative_mapping[c_id] = [f_id, m_id]\n",
    "        for ances_id in relative_mapping[c_id]:\n",
    "            rel_ids.append([c_id, ances_id, 'first-degree'])\n",
    "\n",
    "    # Step 2: 2nd generation\n",
    "    np.random.shuffle(child_ids_1C)  # Shuffle the first-degree children\n",
    "    group_1CF = child_ids_1C[:n//4]  # 50 from the first group\n",
    "    group_1CM = child_ids_1C[n//4:]  # 50 from the second group\n",
    "    child_ids_2C = ['2C_' + str(i) for i in range(n//4)]\n",
    "    for c_id, f_id, m_id in zip(child_ids_2C, group_1CF, group_1CM):\n",
    "        Gdata_n = add_child(Gdata_n, c_id, f_id, m_id)\n",
    "        f_ances_ids = relative_mapping[f_id]\n",
    "        m_ances_ids = relative_mapping[m_id]\n",
    "        relative_mapping[c_id] = f_ances_ids + m_ances_ids\n",
    "        for ansces_id in relative_mapping[c_id]:\n",
    "            rel_ids.append([c_id, ansces_id, 'second-degree'])\n",
    "\n",
    "    # Step 3: 3rd generation\n",
    "    np.random.shuffle(child_ids_2C)  # Shuffle the second-degree children\n",
    "    group_2CF = child_ids_2C[:n//8]\n",
    "    group_2CM = child_ids_2C[n//8:]\n",
    "    child_ids_3C = ['3C_' + str(i) for i in range(n//8)]\n",
    "    for c_id, f_id, m_id in zip(child_ids_3C, group_2CF, group_2CM):\n",
    "        Gdata_n = add_child(Gdata_n, c_id, f_id, m_id)\n",
    "        f_ances_ids = relative_mapping[f_id]\n",
    "        m_ances_ids = relative_mapping[m_id]\n",
    "        for ansces_id in f_ances_ids + m_ances_ids:\n",
    "            rel_ids.append([c_id, ansces_id, 'third-degree'])\n",
    "\n",
    "    # Step 6: Create the relationship DataFrame and save it to a CSV file\n",
    "    rel_ids_df = pd.DataFrame(rel_ids, columns=[\"sampleID\", \"ancestryID\", \"generation\"])\n",
    "\n",
    "    return Gdata_n.T, rel_ids_df\n",
    "\n",
    "def calculate_kinship(genotype_i: np.ndarray, genotype_j: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the KING kinship coefficient between two individuals.\n",
    "\n",
    "    Parameters:\n",
    "    genotype_i (np.ndarray): Genotype data of the first individual.\n",
    "    genotype_j (np.ndarray): Genotype data of the second individual.\n",
    "\n",
    "    Returns:\n",
    "    float: The calculated KING kinship coefficient.\n",
    "    \"\"\"\n",
    "    n11 = np.sum((genotype_i == 1) & (genotype_j == 1))\n",
    "    n02 = np.sum((genotype_i == 2) & (genotype_j == 0))\n",
    "    n20 = np.sum((genotype_i == 0) & (genotype_j == 2))\n",
    "    n1_s = np.sum(genotype_i == 1)\n",
    "    s_1 = np.sum(genotype_j == 1)\n",
    "\n",
    "    if n1_s == 0:\n",
    "        return 0\n",
    "    \n",
    "    phi_ij = (2 * n11 - 4 * (n02 + n20) - n1_s + s_1) / (4 * n1_s)\n",
    "    \n",
    "    if phi_ij > 0.5:\n",
    "        return 0.5\n",
    "    \n",
    "    if phi_ij < 0:\n",
    "        return 0\n",
    "    \n",
    "    return phi_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data = pd.read_csv(\"./allofus_snps.csv\")\n",
    "population_dict = gen_data['population'].value_counts().to_dict()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# parameters\n",
    "n_samples = 200\n",
    "\n",
    "# gen synthetic relatives\n",
    "gen_relatives = {}\n",
    "for population_name, _ in population_dict.items():\n",
    "    print(population_name)\n",
    "    \n",
    "    gdata = gen_data[gen_data['population'] == population_name].set_index('SampleID').iloc[:, 1:].copy()\n",
    "    print(gdata.shape)\n",
    "    gdata_n, rel_ids_df = create_relatives(gdata, n_samples*2)\n",
    "    print(gdata_n.shape, rel_ids_df.shape)\n",
    "    \n",
    "    gen_relatives[population_name] = {\n",
    "        'gdata': gdata_n.reset_index(),\n",
    "        'rel': rel_ids_df\n",
    "    }\n",
    "    \n",
    "# compute kinship\n",
    "results = []\n",
    "for name, item in gen_relatives.items():\n",
    "    print(\"======================================================================\")\n",
    "    print(name)\n",
    "\n",
    "    kinship_df = item['rel']\n",
    "    genotype_df = item['gdata']\n",
    "\n",
    "    def get_kinship(row):\n",
    "        c_geno = genotype_df[genotype_df['SampleID'] == row['sampleID']].iloc[0, 2:]\n",
    "        a_geno = genotype_df[genotype_df['SampleID'] == row['ancestryID']].iloc[0, 2:]\n",
    "\n",
    "        return calculate_kinship(c_geno, a_geno)\n",
    "    \n",
    "    kinship_df['kinship'] = kinship_df.apply(lambda row: get_kinship(row), axis = 1)\n",
    "    results.append(kinship_df)\n",
    "\n",
    "result_df = pd.concat(results, axis = 0)\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_relatives['eur']['rel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_relatives['eur']['gdata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_relatives = n_samples + n_samples//2 + n_samples//4\n",
    "print(total_relatives)\n",
    "\n",
    "relative_snps_datas = [item['gdata'].iloc[-total_relatives:, :] for item in gen_relatives.values()]\n",
    "\n",
    "relative_snp_data = pd.concat(relative_snps_datas, axis = 0)\n",
    "\n",
    "print(relative_snp_data.shape)\n",
    "relative_snp_data.to_csv('./allofus_relative_snps.csv', index = False)\n",
    "relative_snp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Validate Relative SNPs by Kinship\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "result_df['generation'] = result_df['generation'].str.split('-', expand = True)[0]\n",
    "\n",
    "plt.rcParams['text.usetex'] = False\n",
    "plt.rcParams.update({'figure.dpi': '300'})\n",
    "\n",
    "# Set up the plot style\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "\n",
    "# Create the histogram\n",
    "sns.histplot(\n",
    "    data=result_df, x='kinship', hue='generation', kde=False, multiple = 'layer',palette='tab10', stat = 'count',bins = 50,\n",
    "    hue_order=['third','first', 'second']\n",
    ")\n",
    "# Show the plot\n",
    "plt.xlim(0, 0.5)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_gen_df = pd.read_csv('./allofus_snps.csv')\n",
    "print(final_gen_df['population'].value_counts(normalize = True))\n",
    "final_gen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('./allofus_snps_real.csv')\n",
    "final_df['population'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAF Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verfication(gen_data, verbose = 1000):\n",
    "    '''\n",
    "    MAF Comparison between synthetic and real data\n",
    "    '''\n",
    "    \n",
    "    df = gen_data\n",
    "    # Assuming 'df' is your DataFrame\n",
    "    # Identify SNP columns (assuming they start with 'SNP')\n",
    "    snp_columns = df.columns.tolist()[2:]\n",
    "    print(\"#\"*100)\n",
    "    #################################################################################\n",
    "    # Initialize a dictionary to store MAF values\n",
    "    maf_dict = {}\n",
    "\n",
    "    # Iterate over each SNP column\n",
    "    for idx, snp in enumerate(snp_columns):\n",
    "        # Get counts of each genotype (0, 1, 2)\n",
    "        genotype_counts = df[snp].value_counts()\n",
    "\n",
    "        # Extract counts, defaulting to 0 if a genotype is absent\n",
    "        n0 = genotype_counts.get(0, 0)  # Homozygous major allele\n",
    "        n1 = genotype_counts.get(1, 0)  # Heterozygous\n",
    "        n2 = genotype_counts.get(2, 0)  # Homozygous minor allele\n",
    "\n",
    "        # Calculate total number of alleles (2 per individual)\n",
    "        total_alleles = 2 * (n0 + n1 + n2)\n",
    "\n",
    "        # Calculate counts of each allele\n",
    "        major_allele_count = (2 * n0) + n1\n",
    "        minor_allele_count = (2 * n2) + n1\n",
    "\n",
    "        # Calculate allele frequencies\n",
    "        major_allele_freq = major_allele_count / total_alleles\n",
    "        minor_allele_freq = minor_allele_count / total_alleles\n",
    "\n",
    "        # MAF is the frequency of the minor allele\n",
    "        maf = min(major_allele_freq, minor_allele_freq)\n",
    "\n",
    "        # Store the MAF value\n",
    "        maf_dict[snp] = maf\n",
    "        \n",
    "        if idx <= verbose:\n",
    "            print(\"MAF:\", maf)\n",
    "\n",
    "    # Convert the MAF dictionary to a DataFrame for better visualization\n",
    "    maf_df = pd.DataFrame.from_dict(maf_dict, orient='index', columns=['MAF'])\n",
    "    print(\"#\"*100)\n",
    "    #################################################################################\n",
    "    # Print Statistics\n",
    "    distribution = {}\n",
    "    for idx, col in enumerate(gen_data.columns[2:]):\n",
    "        distribution[col] = gen_data.loc[:, col].value_counts().to_dict()\n",
    "        if idx < verbose:\n",
    "            print(idx, distribution[col])\n",
    "    \n",
    "    return maf_df, distribution\n",
    "\n",
    "maf_df, distribution_dict = verfication(final_gen_df, verbose = 5000)\n",
    "maf_df.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maf_df, distribution_dict = verfication(final_gen_df, verbose = 5000)\n",
    "maf_df_real, distribution_dict_real = verfication(final_df, verbose = 100)\n",
    "fig, ax = plt.subplots(1,1,figsize = (6,5))\n",
    "plt.hist(maf_df.values, label = 'synthetic', bins  = 100, alpha = 0.8)\n",
    "plt.hist(maf_df_real.values, label = 'original', bins = 100, alpha = 0.8)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TSNE Visualization\n",
    "\n",
    "Conduct TSNE Visualization to verfiy the distribution of synthetic data and real data for each population\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "def perform_tsne(X, scaler = 'standard', n_components_pca = 20, method = 'pca', \n",
    "                 perplexity = 30, learning_rate = 200, n_iter = 1000, seed = 21):\n",
    "    \n",
    "    #X = X.astype(int)\n",
    "    \n",
    "    if scaler == 'one-hot':\n",
    "        X = X.astype(int)\n",
    "        oh = OneHotEncoder(sparse_output = False)\n",
    "        X_scaled = oh.fit_transform(X)\n",
    "        #X_oh = sparse.csr_matrix(X_oh)\n",
    "    else:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Dimensionality reduction using PCA or TruncatedSVD\n",
    "    start = time.time()\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components_pca, random_state=seed)\n",
    "    elif method == 'svd':\n",
    "        reducer = TruncatedSVD(n_components=n_components_pca, random_state=seed)\n",
    "    else:\n",
    "        raise ValueError(\"Method must be 'pca' or 'svd'\")\n",
    "\n",
    "    X_reduced = reducer.fit_transform(X_scaled)\n",
    "\n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2,random_state=seed, n_jobs=-1, learning_rate=learning_rate, perplexity=perplexity, n_iter=n_iter,\n",
    "               verbose = 0)\n",
    "    X_tsne = tsne.fit_transform(X_reduced)\n",
    "    return X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tnse visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import gower\n",
    "\n",
    "# parameters\n",
    "gen_data = final_gen_df\n",
    "d = 100\n",
    "sample_ratio = 0.3\n",
    "\n",
    "# datas\n",
    "split_indices = []\n",
    "split_indices_real = []\n",
    "Xs,Xreals = [],[]\n",
    "population_names = []\n",
    "cum_index, cum_index_real = 0,0\n",
    "\n",
    "for name, group in gen_data.groupby('population'):\n",
    "    \n",
    "    print(name)\n",
    "    # generated data\n",
    "    print('generated ....')\n",
    "    item = group.values[:, 2:]\n",
    "    np.random.seed(21)\n",
    "    N = int(item.shape[0]*sample_ratio)\n",
    "    sample_indices = np.random.choice(np.arange(N), size = N, replace = False)\n",
    "    item = item[sample_indices, :]\n",
    "    Xs.append(item)\n",
    "    cum_index += item.shape[0]\n",
    "    split_indices.append(cum_index)\n",
    "    \n",
    "    # real data\n",
    "    print('real....')\n",
    "    item_real = final_df[final_df['population'] == name].values[:,1:]\n",
    "    if N <= item_real.shape[0]:\n",
    "        sample_indices = np.random.choice(np.arange(N), size = N, replace = False)\n",
    "        item_real = item_real[sample_indices, :]\n",
    "    Xreals.append(item_real)\n",
    "    cum_index_real += item_real.shape[0]\n",
    "    split_indices_real.append(cum_index_real)\n",
    "    \n",
    "    population_names.append(name)\n",
    "\n",
    "print(split_indices, split_indices_real)\n",
    "    \n",
    "# data\n",
    "X = np.concatenate(Xs, axis=0)\n",
    "X_real = np.concatenate(Xreals, axis = 0)\n",
    "print(X.shape, X_real.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change population name to different population group to see tsne for different ancestry \n",
    "population_name = 'eur'\n",
    "\n",
    "# show tsne\n",
    "Xs = np.split(X, split_indices[:-1])\n",
    "Xreals = np.split(X_real, split_indices_real[:-1])\n",
    "i = poulation_names.index(population_name)\n",
    "X_emb_gen = perform_tsne(Xs[i], perplexity = 200, n_components_pca = 25, method = 'pca', scaler = 'standard', seed = 23)\n",
    "X_emb_real = perform_tsne(Xreals[i], perplexity = 200, n_components_pca = 25, method = 'pca', scaler = 'standard', seed = 23)\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_emb_gen[:, 0], X_emb_gen[:, 1], label=population_names[i], s=50, alpha=0.7, c = 'blue')\n",
    "plt.scatter(X_emb_real[:, 0], X_emb_real[:, 1], label=population_names[i], s=50, alpha=0.7, c = 'red')\n",
    "\n",
    "plt.title(f'{population_names[i]} Plot')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
